+-----------------+--------+
|    Parameter    | Value  |
+=================+========+
| Batch size      | 32     |
+-----------------+--------+
| Dataset         | DD     |
+-----------------+--------+
| Epochs          | 10000  |
+-----------------+--------+
| Exp name        | DD     |
+-----------------+--------+
| Gpu index       | 0      |
+-----------------+--------+
| Improved        | False  |
+-----------------+--------+
| Lr              | 0.0005 |
+-----------------+--------+
| Patience        | 40     |
+-----------------+--------+
| Pooling ratio   | 0.8    |
+-----------------+--------+
| Seed            | 1227   |
+-----------------+--------+
| Test batch size | 1      |
+-----------------+--------+
| Weight decay    | 0.0001 |
+-----------------+--------+
Using GPU: 0
GraphUNets(
  (conv1): GCNConv(89, 32)
  (conv2): GCNConv(32, 64)
  (conv3): GCNConv(64, 128)
  (conv4): GCNConv(128, 256)
  (pool1): GPool(
    (p): Linear(in_features=32, out_features=1, bias=False)
  )
  (pool2): GPool(
    (p): Linear(in_features=64, out_features=1, bias=False)
  )
  (pool3): GPool(
    (p): Linear(in_features=128, out_features=1, bias=False)
  )
  (unpool): GUnpool()
  (conv5): GCNConv(384, 128)
  (conv6): GCNConv(192, 64)
  (conv7): GCNConv(96, 32)
  (ac): ELU(alpha=1.0)
  (l1): Linear(in_features=64, out_features=64, bias=False)
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Model Parameter: 115522
Using Adam

Epoch #000, Train_Loss: [0.6886, 0.6866, 0.6910, 0.6711, 0.6765, 0.6597, 0.6846, 0.6620, 0.6460, 0.6918, 0.6989, 0.6671, 0.7405, 0.7277, 0.7457, 0.6798, 0.6737, 0.6448, 0.6896, 0.6265, 0.6618, 0.7215, 0.6631, 0.6622, 0.7024, 0.6803, 0.6438, 0.6718, 0.6314, 0.7051]
Val_Loss: 0.693126, Val_Acc: 0.521368
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #001, Train_Loss: [0.6369, 0.6734, 0.7154, 0.6686, 0.6918, 0.6854, 0.5623, 0.6303, 0.6593, 0.6986, 0.6455, 0.6840, 0.6944, 0.6383, 0.7330, 0.6500, 0.6358, 0.7084, 0.6419, 0.7232, 0.6839, 0.6492, 0.7165, 0.6639, 0.6731, 0.7099, 0.6898, 0.6433, 0.6462, 0.6986]
Val_Loss: 0.688189, Val_Acc: 0.521368
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #002, Train_Loss: [0.6480, 0.6959, 0.7386, 0.6963, 0.6759, 0.6163, 0.6750, 0.6722, 0.6872, 0.6736, 0.6430, 0.6429, 0.6650, 0.6598, 0.6751, 0.6380, 0.6781, 0.6556, 0.7051, 0.6696, 0.6586, 0.6640, 0.6413, 0.6526, 0.6579, 0.6602, 0.6473, 0.7090, 0.6332, 0.6396]
Val_Loss: 0.690458, Val_Acc: 0.521368

Epoch #003, Train_Loss: [0.7082, 0.7010, 0.6944, 0.6677, 0.6697, 0.6537, 0.6829, 0.6304, 0.6096, 0.6519, 0.6781, 0.6592, 0.6280, 0.6616, 0.6251, 0.6874, 0.6672, 0.6471, 0.7346, 0.6871, 0.6475, 0.5675, 0.6762, 0.6210, 0.6486, 0.6300, 0.6288, 0.6729, 0.7039, 0.5992]
Val_Loss: 0.670036, Val_Acc: 0.529915
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #004, Train_Loss: [0.6248, 0.6721, 0.6357, 0.5706, 0.6024, 0.7016, 0.6620, 0.5850, 0.6769, 0.6139, 0.5853, 0.6457, 0.7128, 0.6379, 0.6624, 0.6399, 0.6297, 0.6822, 0.6531, 0.6763, 0.6255, 0.6243, 0.6226, 0.6005, 0.6182, 0.6212, 0.6321, 0.6575, 0.7028, 0.5256]
Val_Loss: 0.621105, Val_Acc: 0.683761
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #005, Train_Loss: [0.6176, 0.6080, 0.6503, 0.6598, 0.5966, 0.6481, 0.7147, 0.5903, 0.5996, 0.5535, 0.7764, 0.6171, 0.6527, 0.5573, 0.5408, 0.6954, 0.5687, 0.6465, 0.5983, 0.6041, 0.6597, 0.6569, 0.5496, 0.5237, 0.6654, 0.6252, 0.6886, 0.6718, 0.6691, 0.5622]
Val_Loss: 0.605970, Val_Acc: 0.683761
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #006, Train_Loss: [0.6375, 0.5847, 0.5432, 0.6389, 0.5968, 0.6051, 0.6152, 0.6151, 0.6201, 0.5891, 0.5866, 0.5885, 0.5973, 0.5526, 0.7077, 0.6308, 0.6158, 0.6256, 0.5838, 0.5890, 0.5934, 0.5617, 0.6047, 0.5772, 0.5740, 0.5356, 0.5305, 0.6301, 0.6374, 0.5219]
Val_Loss: 0.561568, Val_Acc: 0.743590
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #007, Train_Loss: [0.5866, 0.5539, 0.6065, 0.4800, 0.6031, 0.6983, 0.4266, 0.5536, 0.5070, 0.7219, 0.5642, 0.6267, 0.4899, 0.4960, 0.5421, 0.6315, 0.6355, 0.5421, 0.5892, 0.7031, 0.6821, 0.5905, 0.7081, 0.5515, 0.5419, 0.5048, 0.4460, 0.8055, 0.6305, 0.6408]
Val_Loss: 0.565202, Val_Acc: 0.700855

Epoch #008, Train_Loss: [0.4975, 0.4655, 0.5418, 0.4558, 0.5433, 0.5818, 0.5641, 0.5220, 0.7254, 0.6077, 0.6446, 0.6013, 0.5405, 0.4880, 0.5402, 0.4266, 0.5185, 0.5787, 0.5584, 0.7982, 0.5711, 0.6311, 0.7357, 0.4669, 0.6164, 0.4921, 0.6371, 0.6799, 0.6891, 0.8195]
Val_Loss: 0.570185, Val_Acc: 0.717949

Epoch #009, Train_Loss: [0.6921, 0.6360, 0.5322, 0.6175, 0.5218, 0.5345, 0.6609, 0.5468, 0.6369, 0.5340, 0.5237, 0.5156, 0.6329, 0.5245, 0.5705, 0.5893, 0.6134, 0.5074, 0.5648, 0.5375, 0.6497, 0.5261, 0.6003, 0.5296, 0.5644, 0.6509, 0.4768, 0.5485, 0.6828, 0.6988]
Val_Loss: 0.549307, Val_Acc: 0.743590
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #010, Train_Loss: [0.5685, 0.5491, 0.6227, 0.5637, 0.5260, 0.5757, 0.5439, 0.6270, 0.4891, 0.5471, 0.4551, 0.5073, 0.5223, 0.6078, 0.6689, 0.6376, 0.6065, 0.4609, 0.6759, 0.6018, 0.6419, 0.6489, 0.6270, 0.5335, 0.4905, 0.5115, 0.5781, 0.5468, 0.5701, 0.4877]
Val_Loss: 0.578920, Val_Acc: 0.683761

Epoch #011, Train_Loss: [0.4691, 0.5965, 0.5695, 0.4772, 0.4903, 0.5257, 0.6363, 0.6107, 0.5811, 0.5300, 0.5596, 0.4411, 0.5200, 0.5963, 0.5326, 0.5931, 0.5851, 0.4813, 0.4290, 0.6290, 0.5260, 0.5938, 0.7167, 0.6291, 0.6056, 0.5042, 0.4227, 0.6602, 0.6168, 0.5225]
Val_Loss: 0.523290, Val_Acc: 0.717949
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #012, Train_Loss: [0.5809, 0.4573, 0.4344, 0.8635, 0.6129, 0.5840, 0.5462, 0.6285, 0.5748, 0.5590, 0.5633, 0.6021, 0.6679, 0.5384, 0.4971, 0.6106, 0.5633, 0.5632, 0.5192, 0.5392, 0.6117, 0.4662, 0.5517, 0.4489, 0.6255, 0.6057, 0.4725, 0.6088, 0.4868, 0.4725]
Val_Loss: 0.525050, Val_Acc: 0.735043

Epoch #013, Train_Loss: [0.5029, 0.5284, 0.5373, 0.5402, 0.6211, 0.5141, 0.5962, 0.6391, 0.5749, 0.5265, 0.5474, 0.5213, 0.4811, 0.5380, 0.5342, 0.5253, 0.4183, 0.5843, 0.3784, 0.6215, 0.5416, 0.5215, 0.5705, 0.6477, 0.5176, 0.5054, 0.5411, 0.5682, 0.5358, 0.4486]
Val_Loss: 0.514929, Val_Acc: 0.717949
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #014, Train_Loss: [0.4831, 0.5265, 0.5869, 0.5330, 0.4417, 0.5334, 0.5778, 0.6244, 0.6206, 0.4517, 0.4291, 0.7514, 0.5921, 0.5528, 0.4217, 0.6246, 0.6180, 0.5844, 0.6498, 0.5269, 0.4849, 0.5770, 0.4355, 0.5705, 0.5652, 0.4249, 0.5855, 0.4860, 0.4111, 0.4216]
Val_Loss: 0.545881, Val_Acc: 0.709402

Epoch #015, Train_Loss: [0.5714, 0.5010, 0.3559, 0.5949, 0.7324, 0.5844, 0.5631, 0.6084, 0.4553, 0.6596, 0.5276, 0.4033, 0.5759, 0.5329, 0.4370, 0.4799, 0.4466, 0.5056, 0.6396, 0.3785, 0.6109, 0.5674, 0.7092, 0.5039, 0.5040, 0.4360, 0.5913, 0.5803, 0.5714, 0.6016]
Val_Loss: 0.516933, Val_Acc: 0.735043

Epoch #016, Train_Loss: [0.5917, 0.4487, 0.5336, 0.5841, 0.4556, 0.4201, 0.5124, 0.4355, 0.4061, 0.7317, 0.5901, 0.3430, 0.5836, 0.5749, 0.4675, 0.5074, 0.6173, 0.5510, 0.5490, 0.4678, 0.5264, 0.5489, 0.4629, 0.3976, 0.7438, 0.4583, 0.5611, 0.6943, 0.4751, 0.6730]
Val_Loss: 0.499774, Val_Acc: 0.752137
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #017, Train_Loss: [0.6784, 0.5784, 0.6336, 0.4789, 0.4051, 0.5050, 0.4780, 0.4760, 0.5631, 0.4779, 0.5656, 0.4844, 0.5738, 0.6155, 0.5103, 0.4585, 0.7129, 0.4410, 0.4541, 0.5835, 0.4604, 0.5036, 0.4566, 0.6482, 0.3705, 0.5784, 0.4986, 0.5393, 0.6109, 0.5556]
Val_Loss: 0.513171, Val_Acc: 0.743590

Epoch #018, Train_Loss: [0.5558, 0.5955, 0.5322, 0.4880, 0.5619, 0.5514, 0.6112, 0.4993, 0.5624, 0.6449, 0.5244, 0.5663, 0.4932, 0.5943, 0.5487, 0.4086, 0.6038, 0.4436, 0.5337, 0.4647, 0.4195, 0.5753, 0.5882, 0.4477, 0.4135, 0.5302, 0.6425, 0.4915, 0.4554, 0.5086]
Val_Loss: 0.491105, Val_Acc: 0.743590
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #019, Train_Loss: [0.5293, 0.6887, 0.3843, 0.4677, 0.5330, 0.4931, 0.5730, 0.5447, 0.5178, 0.5924, 0.5154, 0.5646, 0.5593, 0.6034, 0.4406, 0.5032, 0.5495, 0.4227, 0.5090, 0.5340, 0.4187, 0.4309, 0.4925, 0.4318, 0.4464, 0.5578, 0.4383, 0.4764, 0.6287, 0.4081]
Val_Loss: 0.487705, Val_Acc: 0.760684
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #020, Train_Loss: [0.5375, 0.5197, 0.4764, 0.5133, 0.4182, 0.5615, 0.3689, 0.5409, 0.5108, 0.4618, 0.5920, 0.5383, 0.4832, 0.5994, 0.4377, 0.7297, 0.4343, 0.5647, 0.6735, 0.5536, 0.5610, 0.5648, 0.4986, 0.4436, 0.4446, 0.4655, 0.5047, 0.4006, 0.4575, 0.5985]
Val_Loss: 0.483718, Val_Acc: 0.743590
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #021, Train_Loss: [0.3747, 0.4653, 0.6526, 0.4210, 0.3951, 0.3521, 0.5658, 0.4298, 0.3365, 0.5239, 0.6161, 0.7492, 0.5126, 0.5180, 0.5158, 0.5096, 0.5627, 0.5192, 0.5411, 0.4731, 0.4192, 0.5911, 0.4267, 0.5401, 0.5700, 0.5219, 0.4399, 0.5649, 0.6701, 0.4852]
Val_Loss: 0.482831, Val_Acc: 0.760684
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #022, Train_Loss: [0.5620, 0.4584, 0.5049, 0.3994, 0.5797, 0.5365, 0.5514, 0.5089, 0.4425, 0.6428, 0.5784, 0.5509, 0.4920, 0.5097, 0.5348, 0.4545, 0.5598, 0.3678, 0.7484, 0.4616, 0.2425, 0.6246, 0.4441, 0.5180, 0.4745, 0.4421, 0.5178, 0.5051, 0.6205, 0.5951]
Val_Loss: 0.474642, Val_Acc: 0.752137
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #023, Train_Loss: [0.5888, 0.4938, 0.3977, 0.5099, 0.6036, 0.4777, 0.4360, 0.5471, 0.4868, 0.4718, 0.4460, 0.6903, 0.4306, 0.4891, 0.6476, 0.4487, 0.4078, 0.3960, 0.6526, 0.4133, 0.4181, 0.4734, 0.5278, 0.4960, 0.6617, 0.5184, 0.3781, 0.5373, 0.4096, 0.4925]
Val_Loss: 0.469306, Val_Acc: 0.752137
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #024, Train_Loss: [0.4519, 0.3478, 0.3570, 0.3817, 0.5330, 0.4251, 0.5977, 0.6418, 0.6525, 0.4734, 0.3962, 0.6895, 0.4994, 0.4992, 0.5090, 0.5559, 0.3916, 0.4918, 0.5367, 0.5889, 0.4457, 0.5578, 0.3983, 0.3621, 0.4696, 0.5691, 0.4612, 0.3640, 0.4950, 0.8325]
Val_Loss: 0.474637, Val_Acc: 0.769231

Epoch #025, Train_Loss: [0.4532, 0.4263, 0.5881, 0.5463, 0.6593, 0.3946, 0.4645, 0.5430, 0.4798, 0.4708, 0.5835, 0.4515, 0.5148, 0.4371, 0.4625, 0.5346, 0.3877, 0.3607, 0.5405, 0.5324, 0.5857, 0.4724, 0.5211, 0.4956, 0.5435, 0.3685, 0.4423, 0.4215, 0.5456, 0.6576]
Val_Loss: 0.460958, Val_Acc: 0.777778
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #026, Train_Loss: [0.5362, 0.4520, 0.4467, 0.5017, 0.5231, 0.5426, 0.4722, 0.4132, 0.6524, 0.4865, 0.6054, 0.5980, 0.4666, 0.5337, 0.3806, 0.3861, 0.6814, 0.5976, 0.4764, 0.4667, 0.4827, 0.4314, 0.5280, 0.4531, 0.4052, 0.6173, 0.5448, 0.3940, 0.5750, 0.6008]
Val_Loss: 0.465454, Val_Acc: 0.760684

Epoch #027, Train_Loss: [0.4918, 0.5359, 0.5223, 0.4806, 0.4428, 0.5146, 0.4735, 0.4604, 0.4978, 0.3906, 0.3400, 0.6390, 0.4079, 0.6984, 0.5847, 0.4263, 0.4037, 0.5298, 0.5603, 0.4138, 0.3456, 0.3612, 0.3898, 0.4582, 0.5818, 0.4456, 0.4445, 0.5951, 0.6364, 0.4170]
Val_Loss: 0.457743, Val_Acc: 0.752137
The current best model is saved in: ******** outputs/DD/model.pth *********

Epoch #028, Train_Loss: [0.5140, 0.5680, 0.4059, 0.4443, 0.5498, 0.5111, 0.5112, 0.5372, 0.4249, 0.4584, 0.5803, 0.3999, 0.5305, 0.5517, 0.4752, 0.3910, 0.5205, 0.4306, 0.4417, 0.4019, 0.5384, 0.4618, 0.3725, 0.4640, 0.6792, 0.3506, 0.3999, 0.4340, 0.5486, 0.5001]
Val_Loss: 0.463232, Val_Acc: 0.769231

Epoch #029, Train_Loss: [0.4947, 0.3637, 0.3627, 0.4204, 0.3942, 0.4966, 0.6286, 0.5185, 0.5691, 0.5978, 0.5184, 0.4397, 0.5642, 0.5218, 0.5044, 0.4856, 0.2578, 0.5723, 0.5292, 0.5693, 0.5187, 0.4095, 0.4190, 0.4103, 0.4826, 0.3775, 0.5039, 0.4768, 0.3777, 0.6566]
Val_Loss: 0.473238, Val_Acc: 0.752137

Epoch #030, Train_Loss: [0.6134, 0.5583, 0.5136, 0.2780, 0.4942, 0.4357, 0.5162, 0.3910, 0.4799, 0.3534, 0.4140, 0.6656, 0.5127, 0.4822, 0.3044, 0.5627, 0.4914, 0.3929, 0.4159, 0.5277, 0.4049, 0.4048, 0.4011, 0.5472, 0.6346, 0.5512, 0.5089, 0.3961, 0.4920, 0.3830]
Val_Loss: 0.471671, Val_Acc: 0.760684

Epoch #031, Train_Loss: [0.4233, 0.2776, 0.3676, 0.5159, 0.4094, 0.5178, 0.4581, 0.3236, 0.6118, 0.4103, 0.6117, 0.3652, 0.4442, 0.3815, 0.4527, 0.5478, 0.5649, 0.4175, 0.4806, 0.3987, 0.5215, 0.5623, 0.4755, 0.6768, 0.4672, 0.5988, 0.7547, 0.4592, 0.4923, 0.4513]
Val_Loss: 0.509943, Val_Acc: 0.735043

Epoch #032, Train_Loss: [0.4555, 0.5999, 0.4738, 0.3741, 0.6299, 0.4930, 0.4373, 0.4284, 0.4102, 0.4680, 0.5262, 0.3764, 0.3914, 0.3943, 0.4410, 0.4378, 0.5993, 0.5303, 0.5039, 0.4521, 0.4429, 0.4338, 0.4371, 0.6169, 0.6531, 0.2717, 0.4355, 0.4885, 0.6152, 0.3722]
Val_Loss: 0.463996, Val_Acc: 0.752137

Epoch #033, Train_Loss: [0.3661, 0.3421, 0.4089, 0.4312, 0.4287, 0.5484, 0.4185, 0.4001, 0.3879, 0.2805, 0.5692, 0.5439, 0.5626, 0.5662, 0.4568, 0.4277, 0.5181, 0.4439, 0.4990, 0.4455, 0.5396, 0.4304, 0.4731, 0.3971, 0.4171, 0.5424, 0.6288, 0.4848, 0.3415, 0.4305]
Val_Loss: 0.490165, Val_Acc: 0.760684

Epoch #034, Train_Loss: [0.5076, 0.5293, 0.3800, 0.5896, 0.4231, 0.4821, 0.6067, 0.3743, 0.4990, 0.3474, 0.4457, 0.3989, 0.6230, 0.3497, 0.5983, 0.3611, 0.6141, 0.4023, 0.5064, 0.3873, 0.5304, 0.4512, 0.3219, 0.5608, 0.6286, 0.5202, 0.4431, 0.4452, 0.3599, 0.4093]
Val_Loss: 0.474625, Val_Acc: 0.743590

Epoch #035, Train_Loss: [0.6236, 0.5018, 0.4903, 0.3971, 0.4770, 0.5641, 0.4730, 0.4408, 0.3553, 0.3588, 0.4187, 0.6204, 0.4544, 0.3852, 0.3761, 0.4497, 0.4748, 0.3487, 0.3651, 0.4407, 0.3899, 0.3068, 0.3876, 0.4945, 0.5109, 0.2536, 0.6041, 0.4223, 0.7343, 0.7422]
Val_Loss: 0.462061, Val_Acc: 0.769231

Epoch #036, Train_Loss: [0.6364, 0.3351, 0.4659, 0.5286, 0.4471, 0.5786, 0.5087, 0.4410, 0.4261, 0.4459, 0.3866, 0.3390, 0.4922, 0.4527, 0.2770, 0.6066, 0.4507, 0.3246, 0.5563, 0.4837, 0.4332, 0.4132, 0.5587, 0.7082, 0.4414, 0.6034, 0.3619, 0.5495, 0.5493, 0.2812]
Val_Loss: 0.490420, Val_Acc: 0.752137

Epoch #037, Train_Loss: [0.6347, 0.5540, 0.5339, 0.6985, 0.6466, 0.3485, 0.4291, 0.5065, 0.4034, 0.3096, 0.3800, 0.4470, 0.3691, 0.4817, 0.3525, 0.3931, 0.4656, 0.4098, 0.3302, 0.5231, 0.6535, 0.4178, 0.4655, 0.4017, 0.5028, 0.4744, 0.4377, 0.4310, 0.5034, 0.5133]
Val_Loss: 0.469756, Val_Acc: 0.769231

Epoch #038, Train_Loss: [0.5564, 0.3630, 0.4075, 0.2832, 0.3683, 0.6506, 0.5855, 0.2925, 0.3885, 0.5903, 0.3498, 0.3056, 0.7337, 0.4553, 0.4535, 0.4891, 0.6814, 0.5754, 0.3498, 0.6179, 0.4211, 0.3040, 0.4410, 0.5586, 0.3505, 0.5369, 0.6095, 0.4583, 0.5270, 0.3364]
Val_Loss: 0.479875, Val_Acc: 0.777778

Epoch #039, Train_Loss: [0.5669, 0.5435, 0.4290, 0.4411, 0.4808, 0.4072, 0.3665, 0.4688, 0.4690, 0.5683, 0.4553, 0.3890, 0.3862, 0.3645, 0.6159, 0.3811, 0.5664, 0.4831, 0.4099, 0.4673, 0.3263, 0.4473, 0.5238, 0.5113, 0.4455, 0.4512, 0.5017, 0.3038, 0.5226, 0.4055]
Val_Loss: 0.464388, Val_Acc: 0.760684

Epoch #040, Train_Loss: [0.3102, 0.4296, 0.3718, 0.5610, 0.4386, 0.5035, 0.5170, 0.3962, 0.5255, 0.3850, 0.4483, 0.4871, 0.4272, 0.4806, 0.5577, 0.5104, 0.2880, 0.5057, 0.4450, 0.3124, 0.7589, 0.5333, 0.4406, 0.4132, 0.3532, 0.4571, 0.3469, 0.3970, 0.3177, 0.4195]
Val_Loss: 0.467530, Val_Acc: 0.769231

Epoch #041, Train_Loss: [0.5206, 0.4496, 0.2955, 0.4195, 0.4642, 0.4398, 0.4417, 0.3688, 0.4729, 0.4039, 0.4268, 0.3859, 0.5398, 0.4387, 0.4547, 0.4461, 0.5457, 0.3625, 0.4598, 0.4144, 0.6959, 0.3921, 0.4815, 0.6230, 0.3632, 0.4259, 0.4670, 0.2979, 0.5222, 0.4289]
Val_Loss: 0.461623, Val_Acc: 0.786325

Epoch #042, Train_Loss: [0.4282, 0.4330, 0.3592, 0.3509, 0.4516, 0.3074, 0.6821, 0.5306, 0.4504, 0.4122, 0.6218, 0.3631, 0.4768, 0.5758, 0.3545, 0.5757, 0.5893, 0.4716, 0.4179, 0.4375, 0.3924, 0.4616, 0.4735, 0.4195, 0.4317, 0.4244, 0.3425, 0.3363, 0.3729, 0.3946]
Val_Loss: 0.485773, Val_Acc: 0.777778

Epoch #043, Train_Loss: [0.4413, 0.4243, 0.5058, 0.3941, 0.3800, 0.1978, 0.3300, 0.3482, 0.4368, 0.4904, 0.3738, 0.3944, 0.5660, 0.5596, 0.4005, 0.6087, 0.4650, 0.5446, 0.5930, 0.4625, 0.4004, 0.5087, 0.4034, 0.2752, 0.5482, 0.5121, 0.4267, 0.4204, 0.5013, 0.4017]
Val_Loss: 0.459935, Val_Acc: 0.803419

Epoch #044, Train_Loss: [0.4447, 0.4307, 0.6227, 0.3964, 0.4914, 0.3160, 0.4832, 0.4087, 0.3763, 0.4496, 0.3527, 0.5361, 0.2971, 0.3421, 0.4630, 0.4651, 0.4359, 0.3830, 0.5222, 0.4825, 0.4279, 0.3015, 0.5527, 0.3699, 0.2764, 0.3956, 0.4958, 0.4490, 0.5479, 0.4171]
Val_Loss: 0.465990, Val_Acc: 0.786325

Epoch #045, Train_Loss: [0.4312, 0.3996, 0.3779, 0.5068, 0.3506, 0.4246, 0.3609, 0.5548, 0.4020, 0.3266, 0.3590, 0.3961, 0.5710, 0.5242, 0.4390, 0.3190, 0.4561, 0.3242, 0.5572, 0.3211, 0.3707, 0.4834, 0.3408, 0.4187, 0.7445, 0.5507, 0.4598, 0.4876, 0.5235, 0.3432]
Val_Loss: 0.490670, Val_Acc: 0.743590

Epoch #046, Train_Loss: [0.3941, 0.5973, 0.3048, 0.3407, 0.3760, 0.3900, 0.5122, 0.3088, 0.6153, 0.6232, 0.7491, 0.3258, 0.5375, 0.3053, 0.4869, 0.3381, 0.3087, 0.4584, 0.3903, 0.5356, 0.3869, 0.5016, 0.3636, 0.2635, 0.2896, 0.5898, 0.4121, 0.2625, 0.5215, 0.2009]
Val_Loss: 0.479997, Val_Acc: 0.786325

Epoch #047, Train_Loss: [0.4594, 0.4535, 0.3254, 0.5611, 0.2413, 0.5248, 0.4067, 0.5180, 0.2966, 0.3165, 0.5630, 0.3043, 0.6248, 0.4103, 0.3080, 0.6548, 0.5554, 0.4040, 0.3728, 0.4296, 0.3790, 0.4266, 0.4843, 0.3719, 0.3133, 0.4139, 0.5295, 0.4478, 0.3655, 0.3312]
Val_Loss: 0.496018, Val_Acc: 0.760684

Epoch #048, Train_Loss: [0.3881, 0.5515, 0.3754, 0.3520, 0.3373, 0.3744, 0.3416, 0.3840, 0.3383, 0.4131, 0.5252, 0.3060, 0.4223, 0.4344, 0.3008, 0.3365, 0.4297, 0.2756, 0.3778, 0.4425, 0.4480, 0.6161, 0.4370, 0.4763, 0.4584, 0.5150, 0.6256, 0.4043, 0.4710, 0.5207]
Val_Loss: 0.484189, Val_Acc: 0.760684

Epoch #049, Train_Loss: [0.4001, 0.3340, 0.5789, 0.3195, 0.4203, 0.4269, 0.4072, 0.4507, 0.3858, 0.3067, 0.4543, 0.4469, 0.3554, 0.3828, 0.5060, 0.3584, 0.4572, 0.2721, 0.3141, 0.5913, 0.7388, 0.3453, 0.4339, 0.3088, 0.4183, 0.5444, 0.4979, 0.3715, 0.3924, 0.2411]
Val_Loss: 0.481258, Val_Acc: 0.777778

Epoch #050, Train_Loss: [0.3599, 0.4935, 0.2679, 0.4879, 0.4520, 0.3786, 0.4425, 0.4719, 0.3368, 0.4981, 0.5163, 0.3425, 0.2790, 0.3667, 0.4389, 0.3201, 0.4344, 0.3312, 0.5749, 0.3659, 0.3789, 0.4213, 0.4167, 0.2833, 0.3829, 0.3768, 0.4810, 0.6851, 0.3826, 0.8142]
Val_Loss: 0.473689, Val_Acc: 0.786325

Epoch #051, Train_Loss: [0.3141, 0.3705, 0.4825, 0.3347, 0.2898, 0.4476, 0.5413, 0.5909, 0.4666, 0.3671, 0.4542, 0.4500, 0.5642, 0.5130, 0.4503, 0.3693, 0.4486, 0.5434, 0.4574, 0.2759, 0.3228, 0.3354, 0.4245, 0.4084, 0.3131, 0.5736, 0.4393, 0.3613, 0.4379, 0.3033]
Val_Loss: 0.472449, Val_Acc: 0.760684

Epoch #052, Train_Loss: [0.3853, 0.3896, 0.3468, 0.4071, 0.5685, 0.6289, 0.5015, 0.4122, 0.4270, 0.2707, 0.3497, 0.3040, 0.3653, 0.5763, 0.4173, 0.1974, 0.2683, 0.3233, 0.2874, 0.4373, 0.5234, 0.5271, 0.3959, 0.4177, 0.5789, 0.3435, 0.4077, 0.3589, 0.5037, 0.2213]
Val_Loss: 0.492259, Val_Acc: 0.769231

Epoch #053, Train_Loss: [0.4238, 0.3528, 0.4990, 0.4354, 0.4450, 0.2413, 0.3586, 0.4016, 0.5138, 0.4536, 0.4268, 0.3391, 0.4126, 0.5729, 0.4673, 0.4187, 0.4655, 0.3003, 0.3560, 0.4043, 0.4346, 0.4302, 0.3832, 0.4201, 0.3456, 0.4795, 0.4726, 0.4559, 0.3008, 0.2103]
Val_Loss: 0.495271, Val_Acc: 0.777778

Epoch #054, Train_Loss: [0.6004, 0.2735, 0.4448, 0.3999, 0.3220, 0.3757, 0.3137, 0.5557, 0.4472, 0.2402, 0.2305, 0.3410, 0.4050, 0.3693, 0.5153, 0.3712, 0.3654, 0.4436, 0.3689, 0.3187, 0.3005, 0.5525, 0.3899, 0.3841, 0.3420, 0.2201, 0.3278, 0.3024, 0.7735, 0.5867]
Val_Loss: 0.458965, Val_Acc: 0.786325

Epoch #055, Train_Loss: [0.4633, 0.2926, 0.4744, 0.3486, 0.4850, 0.4003, 0.3775, 0.4608, 0.2939, 0.4193, 0.2634, 0.6360, 0.3138, 0.3289, 0.4741, 0.5038, 0.2729, 0.2709, 0.3258, 0.3411, 0.2889, 0.6046, 0.3580, 0.3660, 0.4678, 0.2610, 0.3107, 0.3803, 0.5248, 0.6143]
Val_Loss: 0.465674, Val_Acc: 0.786325

Epoch #056, Train_Loss: [0.5413, 0.2887, 0.3356, 0.2841, 0.3945, 0.4048, 0.3139, 0.3558, 0.3714, 0.4726, 0.4731, 0.3035, 0.3840, 0.4175, 0.3765, 0.3130, 0.4025, 0.5408, 0.4209, 0.4122, 0.5061, 0.2934, 0.4061, 0.3876, 0.4331, 0.4079, 0.3493, 0.2402, 0.3325, 0.4191]
Val_Loss: 0.468278, Val_Acc: 0.811966

Epoch #057, Train_Loss: [0.3550, 0.5026, 0.3135, 0.3933, 0.3431, 0.4676, 0.3770, 0.3673, 0.2813, 0.3062, 0.2661, 0.3355, 0.5370, 0.3659, 0.3647, 0.4576, 0.2607, 0.4064, 0.3777, 0.6302, 0.4680, 0.5286, 0.3673, 0.2625, 0.4282, 0.3676, 0.2933, 0.3183, 0.3130, 0.3143]
Val_Loss: 0.461452, Val_Acc: 0.803419

Epoch #058, Train_Loss: [0.5688, 0.3762, 0.2717, 0.2520, 0.3604, 0.4072, 0.3386, 0.2963, 0.4791, 0.2799, 0.4130, 0.4204, 0.3523, 0.2847, 0.4880, 0.2779, 0.3222, 0.3035, 0.6031, 0.3904, 0.4186, 0.3484, 0.2830, 0.2935, 0.6178, 0.4393, 0.5011, 0.2615, 0.3760, 0.3967]
Val_Loss: 0.533558, Val_Acc: 0.769231

Epoch #059, Train_Loss: [0.3531, 0.3735, 0.3408, 0.3020, 0.2618, 0.3480, 0.6595, 0.3585, 0.3777, 0.3904, 0.3603, 0.4772, 0.3514, 0.3638, 0.4072, 0.2886, 0.3254, 0.4187, 0.2900, 0.3423, 0.5067, 0.3231, 0.3817, 0.3936, 0.3959, 0.3719, 0.4080, 0.3218, 0.3225, 0.4130]
Val_Loss: 0.486523, Val_Acc: 0.794872

Epoch #060, Train_Loss: [0.1974, 0.5468, 0.4572, 0.3142, 0.3061, 0.4787, 0.2924, 0.3348, 0.3136, 0.5048, 0.3561, 0.5073, 0.5484, 0.2982, 0.3192, 0.4766, 0.3668, 0.4394, 0.3882, 0.2819, 0.5023, 0.4965, 0.3743, 0.2629, 0.4286, 0.3229, 0.3464, 0.3570, 0.2017, 0.3416]
Val_Loss: 0.469922, Val_Acc: 0.811966

Epoch #061, Train_Loss: [0.3494, 0.5772, 0.3569, 0.3401, 0.3242, 0.3557, 0.3337, 0.2608, 0.5780, 0.3065, 0.3984, 0.3726, 0.2494, 0.2913, 0.3247, 0.4460, 0.3393, 0.2682, 0.3721, 0.3826, 0.2733, 0.3971, 0.4200, 0.4386, 0.1818, 0.3247, 0.5131, 0.3747, 0.3301, 0.2990]
Val_Loss: 0.466692, Val_Acc: 0.794872

Epoch #062, Train_Loss: [0.2842, 0.4207, 0.2400, 0.3536, 0.2549, 0.3520, 0.3250, 0.3110, 0.3242, 0.5398, 0.3119, 0.2377, 0.4131, 0.3079, 0.2210, 0.3860, 0.4038, 0.4362, 0.3115, 0.2974, 0.5078, 0.2489, 0.3668, 0.4360, 0.2902, 0.4161, 0.3377, 0.5976, 0.4068, 0.2003]
Val_Loss: 0.493162, Val_Acc: 0.777778

Epoch #063, Train_Loss: [0.3367, 0.3188, 0.4021, 0.1804, 0.2999, 0.3247, 0.4218, 0.2459, 0.3505, 0.3645, 0.3965, 0.2852, 0.3168, 0.4126, 0.2218, 0.4139, 0.2967, 0.7310, 0.5428, 0.3299, 0.3796, 0.2956, 0.4118, 0.3464, 0.3932, 0.2190, 0.2714, 0.2995, 0.5535, 0.3422]
Val_Loss: 0.503176, Val_Acc: 0.794872

Epoch #064, Train_Loss: [0.2061, 0.3579, 0.4228, 0.2646, 0.4029, 0.4134, 0.3265, 0.4214, 0.3179, 0.3297, 0.3921, 0.3394, 0.2838, 0.3459, 0.3518, 0.3043, 0.2024, 0.2855, 0.3301, 0.4659, 0.3577, 0.2490, 0.3856, 0.3841, 0.3410, 0.4191, 0.3958, 0.4538, 0.3086, 0.7372]
Val_Loss: 0.528121, Val_Acc: 0.777778

Epoch #065, Train_Loss: [0.2984, 0.2773, 0.4365, 0.5451, 0.2456, 0.3222, 0.3030, 0.3044, 0.3529, 0.2989, 0.2200, 0.2196, 0.3734, 0.2886, 0.4241, 0.2081, 0.4526, 0.5274, 0.4967, 0.3646, 0.3166, 0.2866, 0.2389, 0.3813, 0.3654, 0.3786, 0.3993, 0.3618, 0.2952, 0.1633]
Val_Loss: 0.484534, Val_Acc: 0.769231

Epoch #066, Train_Loss: [0.2989, 0.3384, 0.2622, 0.3696, 0.3369, 0.3915, 0.2631, 0.3811, 0.4159, 0.3116, 0.2639, 0.2757, 0.3242, 0.3987, 0.2389, 0.1807, 0.3164, 0.4371, 0.2769, 0.3924, 0.3969, 0.3692, 0.3951, 0.3094, 0.3595, 0.4770, 0.2995, 0.2672, 0.4987, 0.3436]
Val_Loss: 0.492371, Val_Acc: 0.786325

Epoch #067, Train_Loss: [0.4404, 0.4184, 0.3737, 0.2959, 0.2722, 0.2810, 0.3921, 0.4580, 0.3006, 0.3110, 0.2251, 0.2883, 0.2901, 0.3107, 0.3837, 0.2733, 0.3088, 0.1652, 0.5800, 0.5230, 0.3182, 0.2787, 0.2795, 0.2511, 0.3945, 0.4221, 0.3681, 0.4296, 0.3963, 0.7296]
Val_Loss: 0.522191, Val_Acc: 0.820513

Epoch #068, Train_Loss: [0.1768, 0.3851, 0.4906, 0.4466, 0.4337, 0.2229, 0.3820, 0.3142, 0.2955, 0.3365, 0.3545, 0.3632, 0.4299, 0.2833, 0.2738, 0.2047, 0.3390, 0.2570, 0.1605, 0.4284, 0.3997, 0.2806, 0.3189, 0.4494, 0.2644, 0.3445, 0.3802, 0.3442, 0.2998, 0.3943]
Val_Loss: 0.535657, Val_Acc: 0.803419

********** TEST START **********
Reload Best Model
The current best model is saved in: ******** outputs/DD/model.pth *********
TEST :: Test_Acc: 0.798319
